Distributed Consensus and the Raft Algorithm
Concepts of Programming Languages
16 Nov 2020
Tags: go, programming, concurrent, go routines, channels

Sebastian Macke
Rosenheim Technical University
sebastian.macke@qaware.de
http://www.qaware.de

* About this Lecture: Some Thoughts
- Goal of this lecture is to look at a non trivial example in Go.
- The problem we look at, is language agnostic.
- It can be implemented in almost every language.
- It combines concurrent programming and network programming.
- It contains non trivial logic.
- It is a real world example with high relevance.

* Distributed Programming
- Concurrent System - multiple execution points (threads, processes or via network)
- Distributed System - processes are running on different networked processors
- Parallel System - like a distributed system, but more closely coupled (shared memory or fast bus)

Terms are not always used consistently.

: Nebenl채ufiges System: es gibt mehrere separate Ausf체hrungspunkte (egal ob Threads/Prozess oder 체ber Netzwerk)
: Verteiltes System: Prozesse laufen auf unterschiedlichen Prozessoren, die oft weit auseinderliegen, aber verbunden sind
: Paralleles System: wie verteiltes System, aber enger gekoppelt (gemeinsamer Speicher, schneller Bus)

*
.background ./img/07-parallel-distributed.png 600 _

* Message Passing
Message Passing is invoking behavior on a separate process (not necessarily on the same computer) by sending messages.

Models for exchanging messages:

- point to point (symmetric/asymmetric, synchronous/asynchronous)
- remote procedure calls (RPC, RMI)
- broadcasting and multicasting

: Sender schickt Nachricht an Empf채nger
: symmetrisch: beide kennen sich / asymmetrisch: nur ein eine Richtung
: synchron: blockiert bis Nachricht gelesen
: RPI: wie lokaler Prozeduraufruf

* What is Distributed Consensus?
Distributed consensus is the problem how to achieve consistency in distributed systems. Distributed consensus protocols can be used for distributed databases which should stay consistent:

- The system stays consistent even if some nodes goes down
- The system stays consistent if a network partition occurs (but could get unavailable)
- The system never responds with wrong or inconsistent data (different responses from nodes)

* The CAP Theorem
In a distributed system the following requirements can be met:

- *Consistency*  - Data is the same across the cluster, so you can read or write from/to any node and get the same data.
- *Availability* - The system stays available even if one or more member goes down
- *Partition* *Tolerance* - If parts of the system loose connection to other parts, the system stays alive

Pick Two: Only two requirements can be met -> CP, AP are typical (CA does not exists)
.link https://codahale.com/you-cant-sacrifice-partition-tolerance/

: AP = DNS, Cloud Computing
: CP = Geldautomat
: CA = RDBMS?

* Securing C and P
- All known algorithms use the concept of a quorum to detect network partition problems
- Only the partition with the majority of nodes stay alive to ensure consistency
- All known algorithms use the concept of a Master or Leader node to control replication
- All known algorithms use a replicated log and implement a two phase commit

: Consistency & Partition Tolerance

* Two Phase Commit
Phases:

- Prepare Phase - Data is persisted on all members. Data is not visible but stored.
- Commit Phase - Persisted data will be loaded into memory. Data gets visible.
- This is typically implemented with an append only log (transaction log) and a state machine which reads the log entries and loads them into memory.

* What is Raft?
- Raft is a protocol for distributed consensus
- Raft is designed to be easy understandable
- Raft predecessor Paxos was highly complex
- Most Paxos implementations are buggy or academic 
- Raft was developed 2014 in a phd thesis at Stanford University
- Zookeeper ZAB is an alternative but more complex solution
.link https://raft.github.io/raft.pdf The Raft Paper
.link https://github.com/lshmouse/reading-papers/blob/master/distributed-system/Zab:%20High-performance%20broadcast%20for%0Aprimary-backup%20systems.pdf The ZAB paper
.link https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf The Paxos Paper


* Who uses Raft?
- ectd - The Cloud Native key value store -> Part of Kubernetes!
- Docker Swarm - Docker in cluster mode
- dgraph - A Scalable, Distributed, Low Latency, High Throughput Graph Database
- tikv - A Distributed transactional key value database powered by Rust and Raft
- swarmkit - A toolkit for orchestrating distributed systems at any scale.
- chain core - Software for operating permissioned, multi-asset blockchain networks
- Elastic Search - Distributed Search engine (=document oriented database)
- ...

* The Raft Algorithm
- Raft is based on a replicated state machine with the states: *FOLLOWER*, *CANDIDATE* and *LEADER*
- All nodes start in the FOLLOWER state
- The leader is responsible for sending heartbeat message to all members
- The leader is responsible to replicate data to all other nodes (2 phase commit)
- If a member does not receive a leader message in a random timeout interval, an election starts
- During an election a candidate sends vote messages to all cluster members
- The election is won, if a quorum (>50% = n/2 + 1) of members respond with OK

* Raft in Action
.link http://thesecretlivesofdata.com/raft/ Raft Explanation
.link https://raft.github.io/raftscope/index.html The Raftscope Visualization

* The Raft State Model
- There is only one Leader which is responsible for consistency and replication
.image https://3.bp.blogspot.com/-6hYgzWqZBVU/WoXoq9gg9LI/AAAAAAAAVH0/OojT-PYzg1kzYxBZ8Gqz2QUwkz-5-O_zACLcBGAs/s1600/Screen%2BShot%2B2018-02-15%2Bat%2B3.07.43%2BPM.png

* Implementing Raft with Go
The most prominent implementation is the Raft implementation of Etcd (Part of Kubernetes)
.link https://github.com/etcd-io/etcd/tree/master/raft
This implementation is highly optimized and abstracts from the Raft paper

Other implementations

.link https://github.com/hashicorp/raft
.link https://github.com/search?q=go-raft
.link https://raft.github.io/raft.pdf Read the Raft paper for specification

* Is it possible to build your own Raft implementation?
Requirements and Decisions

- We want to stay as close a possible on the original specification
- We want to make a Raft cluster local testable (as single process)
- We want to use gRPC as middleware
.link https://github.com/s-macke/concepts-of-programming-languages/tree/master/dp/kvstore/core/raft
.link https://github.com/s-macke/concepts-of-programming-languages/tree/master/dp/raft

* Step I - Defining a KV Business API 
.code ../src/distributed/kvstore/kvstore-api.go

- This is the business API for setting and getting data in/out or Raft cluster

* Step II - The Raft Interface : AppendEntries
.code ../src/distributed/kvstore/core/raft/noderpc.go /type NodeRPC/
.code ../src/distributed/kvstore/core/raft/noderpc.go /AppendEntries/,23
- The interface could be easily proxied with gRPC or run locally without proxy

* Step II - The Raft Interface : RequestVote
.code ../src/distributed/kvstore/core/raft/noderpc.go /type NodeRPC/
.code ../src/distributed/kvstore/core/raft/noderpc.go /RequestVote/,37

* Step III - Implement the Raft Interfaces in a Server/Node
.code ../src/distributed/kvstore/core/raft/node.go /type Node/,28

* Step IV - Write Tests 
.code ../src/distributed/kvstore/core/raft/node_test.go /TestElection/,33

* Interesting Parts
- Statemachine 
- Concurrency: Behavoir of remote calls, election and heartbeat timers
- Design: Timer implementation
- Clean Architecture: Separation of Raft and server APIs 
- Clean Code: Testability

* Be Creative!
- Write your own Raft Implementation!
.link https://www.youtube.com/watch?v=YbZ3zDzDnrw More information

* Summary 
- Go is an excellent choice to implement an distributed protocol like Raft
- You can implement the Raft specification with ca 1000-2000 Loc 
- You can learn from the Etcd implementation, which is on Github
- Building a production safe implementation is hard in any language anyway!

: TODO: Exersice: Distributed calculation of the Collatz Conjecture
: TODO Use TCP Stream, HTTP with JSON, HTTP with XML, GRPC
: https://www.youtube.com/watch?v=094y1Z2wpJg
: Extends to blocks to optimize
